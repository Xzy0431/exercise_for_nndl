# 函数拟合实验报告

## 1. 实验背景

神经网络理论表明，一个包含 ReLU 激活函数的两层神经网络可以拟合任何函数。本实验的目标是通过一个简单的两层 ReLU 网络来拟合特定的非线性函数，并验证这一结论。我首先使用Tensorflow框架进行了实验，后续补充了numpy直接实现的版本。

## 2. 目标函数与数据采集

### 2.1 目标函数

我们选择一个带有非线性成分的函数进行拟合：

$$ f(x) = \sin(2\pi x) + 0.5x $$

该函数既包含周期性的正弦项，又有线性项，增加了拟合的难度。

### 2.2 数据集构造

- **数据范围**：$x$ 取值范围为 $[-1,1]$。
- **数据量**：训练集和测试集各包含 100 个均匀采样的数据点。
- **归一化**：对输入数据进行均值归一化，使其均值为 0，标准差为 1，以提升训练稳定性。

## 3. 模型设计

本实验采用一个简单的两层神经网络，结构如下：

- **输入层**：1 维（输入 $x$）。
- **隐藏层**：50 个神经元，激活函数为 ReLU。
- **输出层**：1 维（输出 $y$）。

数学表达式为：

$$ h = \max(0, W_1 x + b_1) $$
$$ \hat{y} = W_2 h + b_2 $$

其中，$W_1$ 和 $W_2$ 是权重矩阵，$b_1$ 和 $b_2$ 是偏置项。

### 3.1 损失函数与优化

- **损失函数**：均方误差（MSE）：
  
  $$ L = \frac{1}{N} \sum (\hat{y} - y)^2 $$

- **优化算法**：Adam 优化器，学习率设为 0.01。

## 4. 训练过程与结果

### 4.1 训练过程

- 训练 5000 轮，并记录损失变化。
- 每 500 轮输出一次损失，以观察训练趋势。
- 训练后，在测试集上进行验证。

### 4.2 拟合效果

最终模型的预测结果与目标函数非常接近：

- 训练损失随轮数逐步下降。
- 预测曲线较好地拟合了目标函数。
- 由于 ReLU 具有分段线性特性，预测曲线在某些区域可能存在轻微的折线。

## 5. 结论与优化方向

### 5.1 主要结论

- 仅使用两层 ReLU 网络，就能较好地拟合目标函数。
- 适当增加隐藏层神经元数和训练轮数，有助于提升拟合效果。
- 归一化输入数据可提升训练的稳定性。

### 5.2 可能的优化方向

- **增加隐藏层**：更深的网络可能提高拟合能力。
- **调整激活函数**：可以尝试 Leaky ReLU 或 Swish，避免 ReLU 死神经元问题。
- **正则化**：添加 L2 正则化或 Dropout，降低过拟合风险。

本实验验证了两层 ReLU 网络可以拟合复杂函数，并为后续优化提供了方向。